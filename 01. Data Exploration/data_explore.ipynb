{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'University_of_Notre_Dame', 'paragra...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Beyoncé', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Montana', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Genocide', 'paragraphs': [{'context...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Antibiotics', 'paragraphs': [{'cont...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  version\n",
       "0  {'title': 'University_of_Notre_Dame', 'paragra...      1.1\n",
       "1  {'title': 'Beyoncé', 'paragraphs': [{'context'...      1.1\n",
       "2  {'title': 'Montana', 'paragraphs': [{'context'...      1.1\n",
       "3  {'title': 'Genocide', 'paragraphs': [{'context...      1.1\n",
       "4  {'title': 'Antibiotics', 'paragraphs': [{'cont...      1.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('../data/squad-v1/archive/train-v1.1.json', orient='column')\n",
    "dev = pd.read_json('../data/squad-v1/archive/dev-v1.1.json', orient='column')\n",
    "df = pd.concat([train, dev], ignore_index=True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showQuestion(titleId, paragraphId, questionId):\n",
    "    \n",
    "    title = df['data'][titleId]['title']\n",
    "    paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "    question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "    answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "    answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "\n",
    "    print('Title')\n",
    "    print(title)\n",
    "    print('\\nParagraph')\n",
    "    print(paragraph)\n",
    "    print('\\nQuestion')\n",
    "    print(question)\n",
    "    print('\\nAnswer')\n",
    "    print(answerStart)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "University_of_Notre_Dame\n",
      "\n",
      "Paragraph\n",
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Question\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer\n",
      "515\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles 490\n",
      "Paragraphs 20963\n",
      "Questions 98169\n"
     ]
    }
   ],
   "source": [
    "titlesCount = len(df['data'])\n",
    "totalParagraphsCount = 0\n",
    "totalQuestionsCount = 0\n",
    "\n",
    "for titleId in range(titlesCount):\n",
    "    paragraphsCount = len(df['data'][titleId]['paragraphs'])\n",
    "    totalParagraphsCount += paragraphsCount\n",
    "    \n",
    "    for paragraphId in range(paragraphsCount):\n",
    "        questionsCount = len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])\n",
    "        totalQuestionsCount += questionsCount\n",
    "        \n",
    "print('Titles', titlesCount)\n",
    "print('Paragraphs', totalParagraphsCount)\n",
    "print('Questions', totalQuestionsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentence(paragraph, answerStart):\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(paragraph)\n",
    "    sentenceStart = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if (sentenceStart + len(sentence) >= answerStart):\n",
    "            return sentence         \n",
    "        \n",
    "        sentenceStart += len(sentence) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    }
   ],
   "source": [
    "paragraph = df['data'][0]['paragraphs'][0]['context']\n",
    "answerStart = df['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "\n",
    "sentence = extractSentence(paragraph, answerStart)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containedInText(text, question):\n",
    "    \n",
    "    questionWords = tokenize.word_tokenize(question.lower())\n",
    "    textWords = tokenize.word_tokenize(text.lower())\n",
    "    wordsContained = 0\n",
    "\n",
    "    for questionWord in questionWords:\n",
    "        for textWord in textWords:\n",
    "            if (questionWord == textWord):\n",
    "                wordsContained += 1\n",
    "                break\n",
    "\n",
    "    return wordsContained / len(questionWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Sentence\n",
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n",
      "\n",
      "Contained\n",
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "question =  df['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "contained = containedInText(sentence, question)\n",
    "\n",
    "print('Question')\n",
    "print(question)\n",
    "print('\\nSentence')\n",
    "print(sentence)\n",
    "print(\"\\nContained\")\n",
    "print(contained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV found. Loading...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def printPercentage(currentStep, maxStep):\n",
    "    stepSize = maxStep / 100\n",
    "    if int(currentStep / stepSize) > int((currentStep - 1) / stepSize):\n",
    "        clear_output()\n",
    "        print('{}%'.format(int(currentStep / stepSize)))\n",
    "\n",
    "questionContainmentCsvName = 'questionContainmentDf.csv'\n",
    "\n",
    "def csvExists(fileName):\n",
    "    file = Path(fileName)\n",
    "    return file.is_file()\n",
    "\n",
    "if csvExists(questionContainmentCsvName):\n",
    "    print(\"CSV found. Loading...\")\n",
    "    questionContainmentDf = pd.read_csv(questionContainmentCsvName)\n",
    "else:\n",
    "    sentenceScore = []\n",
    "    paragraphScore = []\n",
    "\n",
    "    titlesCount = len(df['data'])\n",
    "    for titleId in range(titlesCount):\n",
    "        printPercentage(titleId, titlesCount)\n",
    "\n",
    "        for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "            paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "\n",
    "            for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "                question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "                answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "                answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "                sentence = extractSentence(paragraph, answerStart)\n",
    "\n",
    "                sentenceScore.append(containedInText(sentence, question))\n",
    "                paragraphScore.append(containedInText(paragraph, question))\n",
    "                                        \n",
    "    sentenceScoreDf = pd.DataFrame(sentenceScore, columns=['sentence'])\n",
    "    paragraphScoreDf = pd.DataFrame(paragraphScore, columns=['paragraph'])\n",
    "    questionContainmentDf = pd.concat([sentenceScoreDf, paragraphScoreDf], axis=1)\n",
    "\n",
    "    questionContainmentDf.to_csv(questionContainmentCsvName, index=False)\n",
    "    print(\"Result not found. Generating and saved as CSV...\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    " #Extracting Answers Dataframe\n",
    "answers = []\n",
    "sentences = []\n",
    "\n",
    "titlesCount = len(df['data'])\n",
    "for titleId in range(titlesCount):\n",
    "    printPercentage(titleId, titlesCount)\n",
    "\n",
    "    for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "\n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "            answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "            sentence = extractSentence(paragraph, answerStart)\n",
    "            #For extracting answers\n",
    "            answers.append(answer)\n",
    "            sentences.append(sentence)\n",
    "answerTextsDf = pd.DataFrame(answers, columns=['answer'])\n",
    "sentenceDf = pd.DataFrame(sentences, columns=['sentence'])\n",
    "answersDf = pd.concat([answerTextsDf, sentenceDf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98169.000000</td>\n",
       "      <td>98169.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.463967</td>\n",
       "      <td>0.582192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.190377</td>\n",
       "      <td>0.159048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence     paragraph\n",
       "count  98169.000000  98169.000000\n",
       "mean       0.463967      0.582192\n",
       "std        0.190377      0.159048\n",
       "min        0.000000      0.000000\n",
       "25%        0.333333      0.500000\n",
       "50%        0.461538      0.600000\n",
       "75%        0.600000      0.700000\n",
       "max        1.000000      1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence  paragraph\n",
       "0  0.642857   0.571429\n",
       "1  0.636364   0.636364\n",
       "2  0.533333   0.600000\n",
       "3  0.375000   0.500000\n",
       "4  0.333333   0.416667\n",
       "5  0.272727   0.636364\n",
       "6  0.300000   0.800000\n",
       "7  0.363636   0.727273\n",
       "8  0.000000   0.545455\n",
       "9  0.266667   0.733333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  \n",
       "0  It is a replica of the grotto at Lourdes, Fran...  \n",
       "1  Immediately in front of the Main Building and ...  \n",
       "2  Next to the Main Building is the Basilica of t...  \n",
       "3  Immediately behind the basilica is the Grotto,...  \n",
       "4  Atop the Main Building's gold dome is a golden...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = []\n",
    "\n",
    "for i in range(len(answersDf)):\n",
    "    wordCount.append(len(tokenize.word_tokenize(answersDf.iloc[i]['answer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDf = pd.concat([answersDf, pd.DataFrame(wordCount, columns=['wordCount'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    98169.000000\n",
       "mean         3.355031\n",
       "std          3.731700\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max         46.000000\n",
       "Name: wordCount, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  wordCount  \n",
       "0  It is a replica of the grotto at Lourdes, Fran...          3  \n",
       "1  Immediately in front of the Main Building and ...          5  \n",
       "2  Next to the Main Building is the Basilica of t...          3  \n",
       "3  Immediately behind the basilica is the Grotto,...          7  \n",
       "4  Atop the Main Building's gold dome is a golden...          7  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCount\n",
       "1     32156\n",
       "2     25228\n",
       "3     14348\n",
       "4      7562\n",
       "5      4659\n",
       "6      3051\n",
       "7      2222\n",
       "8      1676\n",
       "9      1206\n",
       "10      975\n",
       "11      755\n",
       "12      652\n",
       "13      566\n",
       "14      461\n",
       "15      407\n",
       "16      313\n",
       "18      274\n",
       "17      269\n",
       "19      244\n",
       "20      191\n",
       "21      182\n",
       "23      138\n",
       "22      131\n",
       "25      120\n",
       "24      101\n",
       "26       77\n",
       "28       59\n",
       "27       58\n",
       "29       28\n",
       "30       19\n",
       "31       12\n",
       "32       11\n",
       "33        6\n",
       "38        2\n",
       "34        2\n",
       "35        2\n",
       "36        2\n",
       "37        2\n",
       "46        1\n",
       "42        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD TYPES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerForWord(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entitiesFound = len(doc.ents)\n",
    "    \n",
    "    if (entitiesFound > 0):\n",
    "        return doc.ents[0].label_\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NerForWord('Portugal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSingleToken(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #The entire text is a single named entity \n",
    "    entitiesFound = len(doc.ents)\n",
    "    if(entitiesFound == 1 and doc.ents[0].text == text):\n",
    "        return True\n",
    "    \n",
    "    #The text is not an named entity, but is a single token\n",
    "    tokensFound = len(doc)\n",
    "    if (tokensFound == 1):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSingleToken('George R. R. Martin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "singleTokenCount = 0\n",
    "\n",
    "\n",
    "for i in range(len(answersDf)):\n",
    "    \n",
    "    printPercentage(i, len(answersDf))\n",
    "    \n",
    "    if (isSingleToken(answersDf.iloc[i]['answer'])):\n",
    "        singleTokenCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508521019873891"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSize = int(len(answersDf))\n",
    "singleTokenCount / sampleSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  wordCount  \\\n",
       "0  It is a replica of the grotto at Lourdes, Fran...          3   \n",
       "1  Immediately in front of the Main Building and ...          5   \n",
       "2  Next to the Main Building is the Basilica of t...          3   \n",
       "3  Immediately behind the basilica is the Grotto,...          7   \n",
       "4  Atop the Main Building's gold dome is a golden...          7   \n",
       "\n",
       "   isSingleToken NER POS TAG DEP shape  isAlpha  isStop  \n",
       "0          False                          False   False  \n",
       "1          False                          False   False  \n",
       "2          False                          False   False  \n",
       "3          False                          False   False  \n",
       "4          False                          False   False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['isSingleToken'] = False\n",
    "answersDf['NER'] = ''\n",
    "answersDf['POS'] = ''\n",
    "answersDf['TAG'] = ''\n",
    "answersDf['DEP'] = ''\n",
    "answersDf['shape'] = ''\n",
    "answersDf['isAlpha'] = False\n",
    "answersDf['isStop'] = False\n",
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def csv_exists(file_path):\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "csv_file_path = \"df.csv\"\n",
    "\n",
    "if csv_exists(csv_file_path):\n",
    "    answersDf = pd.read_csv(csv_file_path)\n",
    "else:\n",
    "    for i in range(len(answersDf)):\n",
    "        answer = answersDf.iloc[i]['answer']\n",
    "        if (isSingleToken(answer)):\n",
    "            answersDf.at[i, 'isSingleToken'] = True\n",
    "            answersDf.at[i, 'NER'] = NerForWord(answer)\n",
    "            \n",
    "            doc = nlp(answer)\n",
    "            \n",
    "            answersDf.at[i, 'POS'] = doc[0].pos_\n",
    "            answersDf.at[i, 'TAG'] = doc[0].tag_\n",
    "            answersDf.at[i, 'DEP'] = doc[0].dep_\n",
    "            answersDf.at[i, 'isAlpha'] = doc[0].is_alpha\n",
    "            answersDf.at[i, 'isStop'] = doc[0].is_stop\n",
    "            \n",
    "            shape = doc[0].shape_\n",
    "            for wordIndex in range(1, len(doc)):\n",
    "                shape += (' ' + doc[wordIndex].shape_)\n",
    "                \n",
    "            answersDf.at[i, 'shape'] = shape\n",
    "\n",
    "    answersDf.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>xxx Xxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  wordCount  \\\n",
       "0  It is a replica of the grotto at Lourdes, Fran...          3   \n",
       "1  Immediately in front of the Main Building and ...          5   \n",
       "2  Next to the Main Building is the Basilica of t...          3   \n",
       "3  Immediately behind the basilica is the Grotto,...          7   \n",
       "4  Atop the Main Building's gold dome is a golden...          7   \n",
       "\n",
       "   isSingleToken  NER  POS TAG  DEP           shape  isAlpha  isStop  \n",
       "0          False                                       False   False  \n",
       "1          False                                       False   False  \n",
       "2           True  ORG  DET  DT  det  xxx Xxxx Xxxxx     True    True  \n",
       "3          False                                       False   False  \n",
       "4          False                                       False   False  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isSingleToken\n",
       "True     49921\n",
       "False    48248\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['isSingleToken'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isStop\n",
       "False    93877\n",
       "True      4292\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['isStop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NER\n",
       "               61309\n",
       "DATE            9421\n",
       "PERSON          7056\n",
       "CARDINAL        6031\n",
       "ORG             5578\n",
       "GPE             3225\n",
       "NORP            1333\n",
       "PERCENT         1252\n",
       "LOC              640\n",
       "MONEY            536\n",
       "ORDINAL          408\n",
       "QUANTITY         384\n",
       "EVENT            295\n",
       "FAC              239\n",
       "TIME             157\n",
       "PRODUCT           83\n",
       "LANGUAGE          79\n",
       "LAW               74\n",
       "WORK_OF_ART       69\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['NER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS\n",
       "         48248\n",
       "PROPN    18094\n",
       "NUM      14158\n",
       "NOUN      9098\n",
       "ADJ       3640\n",
       "VERB      1749\n",
       "DET       1582\n",
       "ADV        663\n",
       "SYM        384\n",
       "ADP        236\n",
       "X          122\n",
       "PRON        58\n",
       "PUNCT       50\n",
       "INTJ        40\n",
       "AUX         28\n",
       "PART        15\n",
       "CCONJ        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['POS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOUNS: Answers are dominated by nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>Zelda</td>\n",
       "      <td>Bringing a dying Midna to Zelda, Link learns h...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40277</th>\n",
       "      <td>Manchester United</td>\n",
       "      <td>In addition, Arsenal and Manchester United dev...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35374</th>\n",
       "      <td>Ricoh</td>\n",
       "      <td>The NES uses a custom-made Picture Processing ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79901</th>\n",
       "      <td>Vitruvius Britannicus</td>\n",
       "      <td>Major architects to promote the change in dire...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38762</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>After the defeat of Anthony and his lover, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      answer  \\\n",
       "2882                   Zelda   \n",
       "40277      Manchester United   \n",
       "35374                  Ricoh   \n",
       "79901  Vitruvius Britannicus   \n",
       "38762                  Egypt   \n",
       "\n",
       "                                                sentence  wordCount  \\\n",
       "2882   Bringing a dying Midna to Zelda, Link learns h...          1   \n",
       "40277  In addition, Arsenal and Manchester United dev...          2   \n",
       "35374  The NES uses a custom-made Picture Processing ...          1   \n",
       "79901  Major architects to promote the change in dire...          2   \n",
       "38762  After the defeat of Anthony and his lover, the...          1   \n",
       "\n",
       "       isSingleToken     NER    POS  TAG       DEP        shape  isAlpha  \\\n",
       "2882            True          PROPN  NNP      ROOT        Xxxxx     True   \n",
       "40277           True     ORG  PROPN  NNP  compound  Xxxxx Xxxxx     True   \n",
       "35374           True          PROPN  NNP      ROOT        Xxxxx     True   \n",
       "79901           True  PERSON  PROPN  NNP  compound  Xxxxx Xxxxx     True   \n",
       "38762           True     GPE  PROPN  NNP      ROOT        Xxxxx     True   \n",
       "\n",
       "       isStop  \n",
       "2882    False  \n",
       "40277   False  \n",
       "35374   False  \n",
       "79901   False  \n",
       "38762   False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'PROPN'].sample(n=5, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>Apple</td>\n",
       "      <td>The iPod is a line of portable media players a...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26904</th>\n",
       "      <td>handicrafts</td>\n",
       "      <td>The state is well known for its handicrafts.</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40513</th>\n",
       "      <td>women</td>\n",
       "      <td>This change is interesting from a sociolinguis...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38140</th>\n",
       "      <td>intermarriage</td>\n",
       "      <td>McNutt says, \"It is probably safe to assume th...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25550</th>\n",
       "      <td>LAME</td>\n",
       "      <td>However, some encoders such as LAME can attach...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              answer                                           sentence  \\\n",
       "2471           Apple  The iPod is a line of portable media players a...   \n",
       "26904    handicrafts       The state is well known for its handicrafts.   \n",
       "40513          women  This change is interesting from a sociolinguis...   \n",
       "38140  intermarriage  McNutt says, \"It is probably safe to assume th...   \n",
       "25550           LAME  However, some encoders such as LAME can attach...   \n",
       "\n",
       "       wordCount  isSingleToken     NER   POS  TAG   DEP  shape  isAlpha  \\\n",
       "2471           1           True     ORG  NOUN   NN  ROOT  Xxxxx     True   \n",
       "26904          1           True          NOUN  NNS  ROOT   xxxx     True   \n",
       "40513          1           True          NOUN  NNS  ROOT   xxxx     True   \n",
       "38140          1           True          NOUN   NN  ROOT   xxxx     True   \n",
       "25550          1           True  PERSON  NOUN   NN  ROOT   XXXX     True   \n",
       "\n",
       "       isStop  \n",
       "2471    False  \n",
       "26904   False  \n",
       "40513   False  \n",
       "38140   False  \n",
       "25550   False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'NOUN'].sample(n=5, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60329</th>\n",
       "      <td>1877</td>\n",
       "      <td>In 1877, the Protestant James Cameron from the...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17947</th>\n",
       "      <td>32</td>\n",
       "      <td>About 2,100 students attend the Edwards Campus...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35419</th>\n",
       "      <td>11.7</td>\n",
       "      <td>The unit itself weighs approximately 11.7 poun...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd.d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65451</th>\n",
       "      <td>1790</td>\n",
       "      <td>The Seine département had been governing Paris...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>47</td>\n",
       "      <td>In 1974, there were 475 institutes of higher e...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90384</th>\n",
       "      <td>12</td>\n",
       "      <td>Amongst these include 5 University of Californ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64351</th>\n",
       "      <td>three</td>\n",
       "      <td>He proposed three worlds: World One, being the...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73146</th>\n",
       "      <td>800 tonnes</td>\n",
       "      <td>The last major attack on London was on 10/11 M...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>ddd xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>131</td>\n",
       "      <td>In 2012, an analysis of the 131 contestants wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31894</th>\n",
       "      <td>1214</td>\n",
       "      <td>John's first wife, Isabel, Countess of Glouces...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           answer                                           sentence  \\\n",
       "60329        1877  In 1877, the Protestant James Cameron from the...   \n",
       "17947          32  About 2,100 students attend the Edwards Campus...   \n",
       "35419        11.7  The unit itself weighs approximately 11.7 poun...   \n",
       "65451        1790  The Seine département had been governing Paris...   \n",
       "13308          47  In 1974, there were 475 institutes of higher e...   \n",
       "90384          12  Amongst these include 5 University of Californ...   \n",
       "64351       three  He proposed three worlds: World One, being the...   \n",
       "73146  800 tonnes  The last major attack on London was on 10/11 M...   \n",
       "7534          131  In 2012, an analysis of the 131 contestants wh...   \n",
       "31894        1214  John's first wife, Isabel, Countess of Glouces...   \n",
       "\n",
       "       wordCount  isSingleToken       NER  POS TAG     DEP     shape  isAlpha  \\\n",
       "60329          1           True      DATE  NUM  CD    ROOT      dddd    False   \n",
       "17947          1           True  CARDINAL  NUM  CD    ROOT        dd    False   \n",
       "35419          1           True  CARDINAL  NUM  CD    ROOT      dd.d    False   \n",
       "65451          1           True      DATE  NUM  CD    ROOT      dddd    False   \n",
       "13308          1           True  CARDINAL  NUM  CD    ROOT        dd    False   \n",
       "90384          1           True  CARDINAL  NUM  CD    ROOT        dd    False   \n",
       "64351          1           True  CARDINAL  NUM  CD    ROOT      xxxx     True   \n",
       "73146          2           True  QUANTITY  NUM  CD  nummod  ddd xxxx    False   \n",
       "7534           1           True  CARDINAL  NUM  CD    ROOT       ddd    False   \n",
       "31894          1           True      DATE  NUM  CD    ROOT      dddd    False   \n",
       "\n",
       "       isStop  \n",
       "60329   False  \n",
       "17947   False  \n",
       "35419   False  \n",
       "65451   False  \n",
       "13308   False  \n",
       "90384   False  \n",
       "64351    True  \n",
       "73146   False  \n",
       "7534    False  \n",
       "31894   False  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'NUM'].sample(n=10, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the school\n",
      "a Catholic character\n",
      "the Main Building's gold dome\n",
      "a golden statue\n",
      "the Virgin Mary\n",
      "front\n",
      "the Main Building\n",
      "it\n",
      "a copper statue\n",
      "Christ\n",
      "arms\n",
      "the legend\n",
      "\"Venite Ad Me Omnes\n",
      "the Main Building\n",
      "the Basilica\n",
      "the Sacred Heart\n",
      "the basilica\n",
      "the Grotto\n",
      "a Marian place\n",
      "prayer\n",
      "reflection\n",
      "It\n",
      "a replica\n",
      "the grotto\n",
      "Lourdes\n",
      "France\n",
      "the Virgin Mary\n",
      "the end\n",
      "the main drive\n",
      "a direct line\n",
      "that\n",
      "3 statues\n",
      "the Gold Dome\n",
      "a simple, modern stone statue\n",
      "Mary\n"
     ]
    }
   ],
   "source": [
    "text = df['data'][0]['paragraphs'][0]['context']\n",
    "doc = nlp(text)\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
